{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionAnalyser:\n",
    "    def __init__(self, path, res=None, missing_data=[], categorical_data=[], label_encoding=[]):\n",
    "        self.dataset = pd.read_csv(path)\n",
    "        self.result = dict()\n",
    "        if res is None:\n",
    "            res = len(self.dataset.columns) - 1\n",
    "        self.X = self.dataset.iloc[:, [i for i in range(len(self.dataset.columns)) if i != res]].values\n",
    "        self.y = self.dataset.iloc[:, res].values\n",
    "\n",
    "        #updated categorical_data and label_encoding\n",
    "        categorical_data = [i - 1 if i > res else i for i in categorical_data]\n",
    "        label_encoding = [i - 1 if i > res else i for i in label_encoding]\n",
    "\n",
    "        # Handling Missing Data\n",
    "        if missing_data:\n",
    "            from sklearn.impute import SimpleImputer\n",
    "            imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            imputer.fit(self.X[:, missing_data])\n",
    "            self.X[:, missing_data] = imputer.transform(self.X[:, missing_data])\n",
    "\n",
    "        # Label Encoding\n",
    "        if label_encoding:\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            for i in label_encoding:\n",
    "                self.X[:, [i]] = le.fit_transform(self.X[:, [i]].reshape(-1)).reshape(-1,1)\n",
    "\n",
    "        # Category Data Encoding\n",
    "        if categorical_data:\n",
    "            from sklearn.compose import ColumnTransformer\n",
    "            from sklearn.preprocessing import OneHotEncoder\n",
    "            ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), categorical_data)], remainder='passthrough')\n",
    "            self.X = np.array(ct.fit_transform(self.X))\n",
    "\n",
    "        # Feature Scaling\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        self.sc_x = StandardScaler()\n",
    "        self.sc_y = StandardScaler()\n",
    "        self.X_sc = self.sc_x.fit_transform(self.X)\n",
    "        self.y_sc = self.sc_y.fit_transform(self.y.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "\n",
    "    def k_fold_cross_validation(self, model, cv=10, xtrain=None, ytrain=None):\n",
    "        if xtrain is None:\n",
    "            xtrain = self.X\n",
    "        if ytrain is None:\n",
    "            ytrain = self.y\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(estimator=model, X=xtrain, y=ytrain, cv=cv, scoring='r2')\n",
    "        return scores.mean(), scores.std()\n",
    "\n",
    "\n",
    "    def grid_search(self, model, param_grid, scoring='r2', cv=10, xtrain=None, ytrain=None):\n",
    "        if xtrain is None:\n",
    "            xtrain = self.X\n",
    "        if ytrain is None:\n",
    "            ytrain = self.y\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=cv, n_jobs=-1)\n",
    "        grid_search.fit(xtrain, ytrain)\n",
    "        return grid_search.best_params_, grid_search.best_score_\n",
    "\n",
    "\n",
    "    def linear_regression(self):\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "\n",
    "        # Create a Linear Regression model instance\n",
    "        model = LinearRegression()\n",
    "\n",
    "        # Define parameters for grid search\n",
    "        linear_regression_parameters = {\n",
    "            'fit_intercept': [True, False],\n",
    "            'copy_X': [True, False],\n",
    "            'positive': [True, False]\n",
    "        }\n",
    "\n",
    "        # Perform grid search to find the best parameters\n",
    "        best_params, best_score = self.grid_search(model, linear_regression_parameters)\n",
    "\n",
    "        # Reassign the model instance with the best parameters found\n",
    "        model = LinearRegression(**best_params)\n",
    "\n",
    "        # Perform k-fold cross-validation with the tuned model\n",
    "        mean_score, std_score = self.k_fold_cross_validation(model)\n",
    "\n",
    "        # Store the result\n",
    "        self.result['linear_regression'] = (mean_score, std_score, best_params)\n",
    "\n",
    "        # Print the results\n",
    "        print('Linear Regression:')\n",
    "        print('Mean R-squared Score:', mean_score)\n",
    "        print('Standard Deviation of R-squared Score:', std_score)\n",
    "        print('Best Parameters:', best_params)\n",
    "\n",
    "        # Return the best parameters for further analysis if needed\n",
    "        return best_params\n",
    "\n",
    "\n",
    "    def polynomial_regression(self):\n",
    "        from sklearn.preprocessing import PolynomialFeatures\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "\n",
    "        best_degree = 0\n",
    "        best_score = float('-inf')  # Initialize with negative infinity\n",
    "        best_std_score = 0  # Initialize with 0\n",
    "\n",
    "        for degree in range(2, 10):\n",
    "            poly_reg = PolynomialFeatures(degree=degree)\n",
    "            X_poly_train = poly_reg.fit_transform(self.X)\n",
    "\n",
    "            model = LinearRegression()\n",
    "            mean_score, std_score = self.k_fold_cross_validation(model, xtrain=X_poly_train)\n",
    "\n",
    "            if mean_score > best_score:\n",
    "                best_score = mean_score\n",
    "                best_std_score = std_score  # Update the best standard deviation\n",
    "                best_degree = degree\n",
    "\n",
    "        # Store the result for the best polynomial degree\n",
    "        self.result['polynomial_regression'] = (best_score, best_std_score, best_degree)\n",
    "\n",
    "        # Print the results\n",
    "        print('Polynomial Regression:')\n",
    "        print('Mean R-squared Score:', best_score)\n",
    "        print('Standard Deviation of R-squared Score:', best_std_score)\n",
    "        print('Best Polynomial Degree:', best_degree)\n",
    "\n",
    "        # Return the best degree for further analysis if needed\n",
    "        return best_degree\n",
    "\n",
    "\n",
    "    def support_vector_regression(self):\n",
    "        from sklearn.svm import SVR\n",
    "\n",
    "        # Initialize SVR model\n",
    "        model = SVR()\n",
    "\n",
    "        # Define parameter grid for grid search\n",
    "        support_vector_regression_parameters = {\n",
    "            'kernel': ['linear', 'poly', 'rbf'],\n",
    "            'degree': [2, 3, 4, 5],\n",
    "        }\n",
    "\n",
    "        # Perform grid search to find the best parameters\n",
    "        best_params, best_score = self.grid_search(model, support_vector_regression_parameters, scoring='r2', xtrain=self.X_sc, ytrain=self.y_sc)\n",
    "\n",
    "        # Reassign the model instance with the best parameters found\n",
    "        model = SVR(**best_params)\n",
    "\n",
    "        # Perform k-fold cross-validation with the tuned model\n",
    "        mean_score, std_score = self.k_fold_cross_validation(model, xtrain=self.X_sc, ytrain=self.y_sc)\n",
    "\n",
    "        # Store the result\n",
    "        self.result['support_vector_regression'] = (mean_score, std_score, best_params)\n",
    "\n",
    "        # Print the results\n",
    "        print('Support Vector Regression:')\n",
    "        print('Mean R-squared Score:', mean_score)\n",
    "        print('Standard Deviation of R-squared Score:', std_score)\n",
    "        print('Best Parameters:', best_params)\n",
    "\n",
    "        # Return the best parameters for further analysis if needed\n",
    "        return best_params\n",
    "\n",
    "\n",
    "    def decision_tree_regression(self):\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "        # Initialize Decision Tree Regressor\n",
    "        model = DecisionTreeRegressor()\n",
    "\n",
    "        # Define parameter grid for grid search\n",
    "        decision_tree_regression_parameters = {\n",
    "            'criterion': ['poisson', 'absolute_error', 'friedman_mse', 'squared_error'],\n",
    "        }\n",
    "\n",
    "        # Perform grid search to find the best parameters\n",
    "        best_params, best_score = self.grid_search(model, decision_tree_regression_parameters, scoring='r2')\n",
    "\n",
    "        # Reassign the model instance with the best parameters found\n",
    "        model = DecisionTreeRegressor(**best_params)\n",
    "\n",
    "        # Perform k-fold cross-validation with the tuned model\n",
    "        mean_score, std_score = self.k_fold_cross_validation(model)\n",
    "\n",
    "        # Store the result\n",
    "        self.result['decision_tree_regression'] = (mean_score, std_score, best_params)\n",
    "\n",
    "        # Print the results\n",
    "        print('Decision Tree Regression:')\n",
    "        print('Mean R-squared Score:', mean_score)\n",
    "        print('Standard Deviation of R-squared Score:', std_score)\n",
    "        print('Best Parameters:', best_params)\n",
    "\n",
    "        # Return the best parameters for further analysis if needed\n",
    "        return best_params\n",
    "\n",
    "\n",
    "    def random_forest_regression(self):\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "        # Initialize Random Forest Regressor\n",
    "        model = RandomForestRegressor()\n",
    "\n",
    "        # Define parameter grid for grid search\n",
    "        random_forest_regression_parameters = {\n",
    "            'criterion': ['poisson', 'absolute_error', 'friedman_mse', 'squared_error']  # Update the criterion parameter\n",
    "        }\n",
    "\n",
    "\n",
    "        # Perform grid search to find the best parameters\n",
    "        best_params, best_score = self.grid_search(model, random_forest_regression_parameters, scoring='r2')\n",
    "\n",
    "        # Reassign the model instance with the best parameters found\n",
    "        model = RandomForestRegressor(**best_params)\n",
    "\n",
    "        # Perform k-fold cross-validation with the tuned model\n",
    "        mean_score, std_score = self.k_fold_cross_validation(model)\n",
    "\n",
    "        # Store the result\n",
    "        self.result['random_forest_regression'] = (mean_score, std_score, best_params)\n",
    "\n",
    "        # Print the results\n",
    "        print('Random Forest Regression:')\n",
    "        print('Mean R-squared Score:', mean_score)\n",
    "        print('Standard Deviation of R-squared Score:', std_score)\n",
    "        print('Best Parameters:', best_params)\n",
    "\n",
    "        # Return the best parameters for further analysis if needed\n",
    "        return best_params\n",
    "\n",
    "\n",
    "    def XGBoost(self):\n",
    "        from xgboost import XGBRegressor\n",
    "\n",
    "        # Initialize XGBoost Regressor\n",
    "        model = XGBRegressor()\n",
    "\n",
    "        # Define parameter grid for grid search\n",
    "        xgboost_parameters = {\n",
    "            'booster': ['gbtree', 'gblinear', 'dart'],\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        }\n",
    "\n",
    "        # Perform grid search to find the best parameters\n",
    "        best_params, best_score = self.grid_search(model, xgboost_parameters, scoring='r2')\n",
    "\n",
    "        # Reassign the model instance with the best parameters found\n",
    "        model = XGBRegressor(**best_params)\n",
    "\n",
    "        # Perform k-fold cross-validation with the tuned model\n",
    "        mean_score, std_score = self.k_fold_cross_validation(model)\n",
    "\n",
    "        # Store the result\n",
    "        self.result['XGBoost'] = (mean_score, std_score, best_params)\n",
    "\n",
    "        # Print the results\n",
    "        print('XGBoost:')\n",
    "        print('Mean R-squared Score:', mean_score)\n",
    "        print('Standard Deviation of R-squared Score:', std_score)\n",
    "        print('Best Parameters:', best_params)\n",
    "\n",
    "        # Return the best parameters for further analysis if needed\n",
    "        return best_params\n",
    "\n",
    "\n",
    "    def CatBoost(self):\n",
    "        from catboost import CatBoostRegressor\n",
    "\n",
    "        # Initialize CatBoost Regressor\n",
    "        model = CatBoostRegressor()\n",
    "\n",
    "        # Define parameter grid for grid search\n",
    "        catboost_parameters = {\n",
    "            'learning_rate': [0.01, 0.1, 0.3]\n",
    "        }\n",
    "\n",
    "        # Perform grid search to find the best parameters\n",
    "        best_params, best_score = self.grid_search(model, catboost_parameters, scoring='r2')\n",
    "\n",
    "        # Reassign the model instance with the best parameters found\n",
    "        model = CatBoostRegressor(**best_params)\n",
    "\n",
    "        # Perform k-fold cross-validation with the tuned model\n",
    "        mean_score, std_score = self.k_fold_cross_validation(model)\n",
    "\n",
    "        # Store the result\n",
    "        self.result['CatBoost'] = (mean_score, std_score , best_params)\n",
    "\n",
    "        # Print the results\n",
    "        print('CatBoost:')\n",
    "        print('Mean R-squared Score:', mean_score)\n",
    "        print('Standard Deviation of R-squared Score:', std_score)\n",
    "        print('Best Parameters:', best_params)\n",
    "\n",
    "        # Return the best parameters for further analysis if needed\n",
    "        return best_params\n",
    "\n",
    "\n",
    "    def analyse(self):\n",
    "        self.linear_regression()\n",
    "        self.polynomial_regression()\n",
    "        self.support_vector_regression()\n",
    "        self.decision_tree_regression()\n",
    "        self.random_forest_regression()\n",
    "        self.XGBoost()\n",
    "        self.CatBoost()\n",
    "        from pprint import pprint\n",
    "        pprint(self.result)\n",
    "\n",
    "\n",
    "    def plot(self):\n",
    "        for key,value in self.result.items():\n",
    "            plt.scatter(value[0], value[1], label=key)\n",
    "        plt.title('R-squared Score vs Standard Deviation')\n",
    "        plt.xlabel('Mean R-squared Score')\n",
    "        plt.ylabel('Standard Deviation of R-squared Score')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = RegressionAnalyser(r'datasets\\Housing.csv',0,categorical_data=[12],label_encoding=[5,6,7,8,9,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser = RegressionAnalyser(r'datasets\\Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.analyse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyser.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
